train1<-train1[, !drops, with = FALSE]
test1<-test1[, !drops, with = FALSE]
#Fix game status columns in train1 to remove null values
train1[train1==""]<-"None"
train1$Wed[is.na(train1$Wed)]<-"None"
train1$Thu[is.na(train1$Thu)]<-"None"
train1$Fri[is.na(train1$Fri)]<-"None"
train1$GameStatus[is.na(train1$GameStatus)]<-"None"
#Fix game status columns in test1 to remove null values
test1$Wed[is.na(test1$Wed)]<-"None"
test1$Thu[is.na(test1$Thu)]<-"None"
test1$Fri[is.na(test1$Fri)]<-"None"
test1$GameStatus[is.na(test1$GameStatus)]<-"None"
test1[test1==""]<-"None"
#Find zero variance columns and remove them
summaryvar<-nearZeroVar(train1,saveMetrics = T)
injurycols<-rownames(summaryvar)
summaryvar<-data.table(summaryvar)
summaryvar$columnames<-injurycols
zerovarcols<-summaryvar[zeroVar==T]
novardrops<-zerovarcols$columnames
train1<-train1[, !novardrops, with = FALSE]
test1<-test1[, !novardrops, with = FALSE]
#Create column to discern between train and test data for after they are split
train1$train<-1
test1$train<-0
#Create master set of train and test
master<-rbind(train1,test1)
#Create categorical levels for variables before dummy creation
col_type<-sapply(master, class)
char_cols<-names(col_type[col_type=="character"])
master[,(char_cols):=lapply(.SD, as.factor),.SDcols=char_cols]
#Split factorized data back into train and test for dummy creation
m_test<-master[train==0]
m_train<-master[train==1]
#Remove train column as it is no longer needed
m_test$train<-NULL
m_train$train<-NULL
master$train<-NULL
#Need to keep position in as a variable and create dummies for this purpose
dummies<-dummyVars(PPRFantasyPoints~., data=master)
library(Metrics)
library(caret)
library(data.table)
library(glmnet)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
library(Metrics)
library(caret)
library(data.table)
library(glmnet)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
#Make dummy variables
dummies <- dummyVars(DKpoints ~ ., data = modeldata[,.(PredPoints,DKsalary,DKpoints)])
train_dummies = predict(dummies, newdata = train[,.(PredPoints,DKsalary,DKpoints)])
test_dummies = predict(dummies, newdata = test[,.(PredPoints,DKsalary,DKpoints)])
x = as.matrix(train_dummies)
y_train = train$DKpoints
x_test = as.matrix(test_dummies)
y_test = test$DKpoints
#Cross validation
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas, na.action = na.omit)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
#Training
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = optimal_lambda)
summary(ridge_reg)
predictions = predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
# Prediction and evaluation on test data
eval_results(y_test, predictions, test)
plot(riridge_reg)
plot(ridge_reg)
predictions = predict(ridge_reg, s = optimal_lambda, newx = x_test, type = "coefficient")
predict(ridge_reg, s = optimal_lambda, newx = x_test, type = "coefficient")
#Training
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda = optimal_lambda)
#Cross validation
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, na.action = na.omit)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
#Training
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda = optimal_lambda)
summary(ridge_reg)
predict(ridge_reg, s = optimal_lambda, newx = x_test, type = "coefficient")
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
# Prediction and evaluation on test data
eval_results(y_test, predictions, test)
optimal_lambda
library(data.table)
#Read in player data
qb<-fread("./project/volume/data/teamconstruction/roll5_final_QB.csv")
te<-fread("./project/volume/data/teamconstruction/roll5_final_TE.csv")
wr<-fread("./project/volume/data/teamconstruction/roll5_final_WR.csv")
rb<-fread("./project/volume/data/teamconstruction/roll5_final_RB.csv")
final<-data.table()
#Merge all player data into one final dataframe
final<-rbind(final,qb,te,wr,rb)
#Read in draftkings data
DKdata<-fread("./project/volume/data/interim/DKsalaries.csv")
#Drop unimportant columns
DKdrops<-c("GID","Team","h/a","Oppt")
DKdata<-DKdata[, !DKdrops, with = FALSE]
#Rename columns in draftkings data to ease merging process
setnames(DKdata, c("Week","Year","Name"), c("week","season","Player"))
#Set keys, merge the two datasets together
setkey(final,season,week,Player,Pos)
setkey(DKdata,season,week,Player,Pos)
modelready<-merge(final,DKdata, all.x = TRUE)
View(modelready)
modelready$PPD<-(modelready$PredPoints/modelready$`DK salary`)*1000
View(modelready)
View(modelready)
#Write out data
fwrite(modelready, "./project/volume/data/teamconstruction/packingdata.csv")
library(Metrics)
library(caret)
library(data.table)
library(glmnet)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
#Make dummy variables
dummies <- dummyVars(DKpoints ~ ., data = modeldata[,.(PredPoints,DKsalary,DKpoints)])
train_dummies = predict(dummies, newdata = train[,.(PredPoints,DKsalary,DKpoints)])
test_dummies = predict(dummies, newdata = test[,.(PredPoints,DKsalary,DKpoints)])
x = as.matrix(train_dummies)
y_train = train$DKpoints
x_test = as.matrix(test_dummies)
y_test = test$DKpoints
#Cross validation
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, na.action = na.omit)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
#Training
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda = optimal_lambda)
summary(ridge_reg)
predictions = predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
# Prediction and evaluation on test data
eval_results(y_test, predictions, test)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
# Prediction and evaluation on test data
eval_results(y_test, predictions, test)
plot_glmnet(ridge_reg)
> ?plot_glmnet
?plot_glmnet
??plot.glmnet
plot(ridge_reg, xvar = "lambda")
predictions = predict(ridge_reg, s = optimal_lambda, newx = x_test, type = "coefficients")
predictions
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
#Make dummy variables
dummies <- dummyVars(DKpoints ~ ., data = modeldata[,.(PredPoints,DKsalary,DKpoints)])
train_dummies1 = predict(dummies, newdata = train[,.(PredPoints,DKpoints)])
#Make dummy variables
dummies1 <- dummyVars(DKpoints ~ ., data = modeldata[,.(PredPoints,DKpoints)])
dummies2<-dummyVars(DKpoints ~ ., data = modeldata[,.(DKsalary,DKpoints)])
train_dummies1 = predict(dummies, newdata = train[,.(PredPoints,DKpoints)])
train_dummies1 = predict(dummies1, newdata = train[,.(PredPoints,DKpoints)])
test_dummies1 = predict(dummies1, newdata = test[,.(PredPoints,DKpoints)])
train_dummies2 = predict(dummies2, newdata = train[,.(DKsalary,DKpoints)])
test_dummies2 = predict(dummies2, newdata = test[,.(DKsalary,DKpoints)])
x1 = as.matrix(train_dummies1)
y_train1 = train$DKpoints
x_test1 = as.matrix(test_dummies1)
y_test1 = test$DKpoints
#Cross validation
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x1, y_train1, alpha = 1, lambda = lambdas, na.action = na.omit)
x1
cv_ridge <- cv.glmnet(x1, y_train1, alpha = 1, lambda = lambdas, na.action = na.omit)
library(Metrics)
library(caret)
library(data.table)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
#Make dummy variables
dummies <- dummyVars(DKpoints ~ ., data = modeldata[,.(PredPoints,DKpoints)])
train_dummies = predict(dummies, newdata = train[,.(PredPoints,DKpoints)])
test_dummies = predict(dummies, newdata = test[,.(PredPoints,DKpoints)])
y_train = train$DKpoints
y_test = test$DKpoints
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
##########################################
#Model to examine PredPoints and DKPoints
##########################################
y_train = train$DKpoints
y_test = test$DKpoints
linearmodel1<-lm(DKpoints~PredPoints, data = train)
summary(linearmodel1)
predictions1<-predict(linearmodel1, test)
plot(linearmodel1)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
results1<-eval_results(y_test, predictions1, test)
results1
summary(linearmodel1)
linearmodel2<-lm(DKsalary~PredPoints, data = train)
summary(linearmodel2)
linearmodel2<-lm(DKpoints~DKsalary, data = train)
summary(linearmodel2)
plot(linearmodel2)
predictions2<-predict(linearmodel2, test)
results2<-eval_results(y_test, predictions2, test)
results2
results1
library(Metrics)
library(caret)
library(data.table)
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
y_train = train$DKpoints
y_test = test$DKpoints
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
linearmodel1<-lm(DKpoints~PredPoints, data = train)
summary(linearmodel1)
plot(linearmodel1)
predictions1<-predict(linearmodel1, test)
results1<-eval_results(y_test, predictions1, test)
results1
linearmodel2<-lm(DKpoints~DKsalary, data = train)
summary(linearmodel2)
plot(linearmodel2)
predictions2<-predict(linearmodel2, test)
results2<-eval_results(y_test, predictions2, test)
results2
results1
results2
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
y_train = train$DKpoints
y_test = test$DKpoints
#Function to calculate rmse and Rsquared
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
##########################################
#Model to examine PredPoints and DKPoints
##########################################
linearmodel1<-lm(DKpoints~PredPoints, data = train)
summary(linearmodel1)
plot(linearmodel1)
##########################################
#Model to examine Dksalary and DKPoints
##########################################
linearmodel2<-lm(DKpoints~DKsalary, data = train)
summary(linearmodel2)
plot(linearmodel2)
results2
#Read in  data
data<-fread("./project/volume/data/teamconstruction/packingdata.csv")
modeldata<-data[,.(PredPoints,`DK salary`,`DK points`)]
setnames(modeldata, c("DK points","DK salary"), c("DKpoints","DKsalary"))
modeldata[is.na(modeldata$PredPoints)]$PredPoints<-0
modeldata[is.na(modeldata$DKsalary)]$DKsalary<-0
modeldata[is.na(modeldata$DKpoints)]$DKpoints<-0
#Set seed for randomization
set.seed(123)
#creating index so data can be split into train and test
trainIndex <- createDataPartition(modeldata$DKpoints,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
train <- modeldata[trainIndex,] #training data (75% of data)
test <- modeldata[-trainIndex,] #testing data (25% of data)
#Preprocess numeric columns
pre_proc_val <- preProcess(train, method = c("center", "scale"))
train = predict(pre_proc_val, train)
test = predict(pre_proc_val, test)
y_train = train$DKpoints
y_test = test$DKpoints
#Function to calculate rmse and Rsquared
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
##########################################
#Model to examine PredPoints and DKPoints
##########################################
linearmodel1<-lm(DKpoints~PredPoints, data = train)
summary(linearmodel1)
#plot(linearmodel1)
predictions1<-predict(linearmodel1, test)
results1<-eval_results(y_test, predictions1, test)
##########################################
#Model to examine Dksalary and DKPoints
##########################################
linearmodel2<-lm(DKpoints~DKsalary, data = train)
summary(linearmodel2)
#plot(linearmodel2)
predictions2<-predict(linearmodel2, test)
results2<-eval_results(y_test, predictions2, test)
results1
results2
